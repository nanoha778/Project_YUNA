import json
import networkx as nx
from itertools import product
import random
import os
from gensim.models import KeyedVectors



# === ÊÑèÂë≥„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØË™≠„ÅøËæº„Åø ===
def load_meaning_network(json_path: str) -> nx.Graph:
    G = nx.Graph()
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
        for src, neighbors in data.items():
            for tgt, weight in neighbors.items():
                try:
                    G.add_edge(src, tgt, weight=float(weight))
                except ValueError:
                    continue
    return G



# Word2Vec„É¢„Éá„É´„ÅÆË™≠„ÅøËæº„ÅøÔºà„Ç∞„É≠„Éº„Éê„É´„Å´1Â∫¶Ôºâ
w2v_model = KeyedVectors.load_word2vec_format("model.vec", binary=False)

# Èñ¢ÈÄ£Ë™û„Çí5Ë™û„Å†„ÅëÂèñÂæó
def get_related_words_from_w2v(word, w2v_model, topn=5):
    try:
        results = w2v_model.most_similar(positive=[word], topn=topn)
        return {w: float(sim) for w, sim in results}
    except KeyError:
        return {}


# === ÂàÜÂâ≤Èñ¢Êï∞ ===
def segment_text_by_meaning_network(
    text: str,
    graph: nx.Graph,
    n_min: int = 1,
    n_max: int = 6,
    related_threshold: float = 0.45,
    similarity_threshold: float = 0.6
) -> list[str]:
    def get_filtered_related_terms(term: str) -> set:
        if term not in graph:
            return set()
        return {
            neighbor
            for neighbor in graph.neighbors(term)
            if graph[term][neighbor].get("weight", 1.0) > related_threshold
        }

    all_matrices = []
    for n in range(n_min, n_max + 1):
        ngrams = [text[i:i+n] for i in range(len(text) - n + 1)]
        matrix = [get_filtered_related_terms(term) for term in ngrams]
        all_matrices.append(matrix)

    max_len = max(len(m) for m in all_matrices)
    break_scores = []
    for i in range(max_len - 1):
        similarities = []
        for matrix in all_matrices:
            if i < len(matrix) - 1:
                a, b = matrix[i], matrix[i+1]
                if a and b:
                    inter = a & b
                    union = a | b
                    sim = len(inter) / len(union) if union else 0.0
                    similarities.append(sim)
        break_score = 1.0 - (sum(similarities) / len(similarities)) if similarities else 1.0
        break_scores.append(break_score)

    result = []
    buffer = text[0]
    for i in range(1, len(text)):
        if i-1 < len(break_scores) and break_scores[i-1] >= similarity_threshold:
            result.append(buffer)
            buffer = text[i]
        else:
            buffer += text[i]
    result.append(buffer)

    return result

def random_chain_evaluation(graph, start_terms, steps=20, related_threshold=0.35):
    result = {}

    for start in start_terms:
        if start not in graph:
            continue

        valid_ends = []

        try:
            neighbors1 = list(graph.neighbors(start))
        except:
            continue  # ÁÑ°Âäπ„Å™Ë™û

        for neighbor in neighbors1:
            current = neighbor
            for _ in range(steps):
                try:
                    next_neighbors = list(graph.neighbors(current))
                    if not next_neighbors:
                        break
                    current = random.choice(next_neighbors)
                except:
                    break

            # ÊúÄÂæå„ÅÆ current „Åå start „Å´„Å§„Å™„Åå„Å£„Å¶„ÅÑ„Çå„Å∞Èáç„ÅøË©ï‰æ°
            if graph.has_edge(start, current):
                weight = graph[start][current].get("weight", 0)
                if weight >= related_threshold:
                    valid_ends.append((current, round(weight, 3)))
                if not graph.has_edge(start, current):
                    graph.add_edge(start, current, weight=weight)
                elif weight > graph[start][current]["weight"]:
                    graph[start][current]["weight"] = weight

        result[start] = valid_ends

    return result

def evaluate_term_scores(
    targets: list[str],        # Ë©ï‰æ°ÂØæË±°„ÅÆË™û„É™„Çπ„Éà
    reference_terms: list[str],  # Ë©ï‰æ°È†ÖÁõÆÔºà‰∏≠ÂøÉË™û„Å™„Å©Ôºâ
    G: nx.Graph,                # ÊÑèÂë≥„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ
    w2v_model,                  # Word2Vec„É¢„Éá„É´
    base_weight: float = 0.35   # Â∑ÆÂàÜË©ï‰æ°„ÅÆÂü∫Ê∫ñÁÇπÔºà‰∏≠ÂøÉÊÄßÂü∫Ê∫ñÔºâ
) -> dict[str, float]:
    """
    ÂêÑÂØæË±°Ë™û„Å´„Å§„ÅÑ„Å¶„ÄÅË©ï‰æ°Ë™ûÔºà‰∏≠ÂøÉË™û„Å™„Å©Ôºâ„Å®„ÅÆÈñ¢ÈÄ£Â∫¶„Å®„ÅÆÂ∑ÆÂàÜÔºàdiffÔºâ„ÇíË©ï‰æ°„Çπ„Ç≥„Ç¢„Å®„Åô„Çã„ÄÇ
    „Çπ„Ç≥„Ç¢„Åå‰Ωé„ÅÑ„Åª„Å©„Äå‰∏≠ÂøÉÊÄß„ÅåÈ´ò„ÅÑÔºàÈáçË¶ÅÔºâ„Äç„Å®Âà§Êñ≠„Åï„Çå„Çã„ÄÇ

    Returns:
        dict[str, float]: {Ë™û: „Çπ„Ç≥„Ç¢}Ôºà„Çπ„Ç≥„Ç¢„ÅåÂ∞è„Åï„ÅÑ„Åª„Å©ÈáçË¶ÅÔºâ
    """
    scores = {}

    for term in targets:
        diffs = []
        for ref in reference_terms:
            # „Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ or W2V „Åã„ÇâÈáç„ÅøÂèñÂæó
            if G.has_edge(term, ref):
                weight = G[term][ref].get("weight", 0.0)
            else:
                weight = get_related_words_from_w2v(term, w2v_model).get(ref, 0.0)

            # ÈñæÂÄ§„Å®„ÅÆÂ∑ÆÂàÜ„ÇíË®àÁÆó
            diff = abs(weight - base_weight)
            diffs.append(diff)

        # Âπ≥Âùádiff„Çπ„Ç≥„Ç¢Ôºà‰∏≠ÂøÉË™û„Å®„ÅÆË∑ùÈõ¢„ÅÆÂêàË®àÔºâ
        scores[term] = sum(diffs) / len(diffs) if diffs else float("inf")

    return scores



def estimate_similarity_by_unicode(unknown_term, known_terms, topn=5):
    def char_distance(c1, c2):
        return abs(ord(c1) - ord(c2))

    def term_distance(t1, t2):
        min_len = min(len(t1), len(t2))
        distances = [char_distance(t1[i], t2[i]) for i in range(min_len)]
        return sum(distances) / len(distances)

    similarity_scores = []
    for known in known_terms:
        dist = term_distance(unknown_term, known)
        similarity = 1 / (1 + dist)  # Ë∑ùÈõ¢„ÇíÈ°û‰ººÂ∫¶„Å´Â§âÊèõ
        similarity_scores.append((known, similarity))

    # È°û‰ººÂ∫¶„ÅÆÈ´ò„ÅÑÈ†Ü„Å´‰∏¶„Åπ„Çã
    similarity_scores.sort(key=lambda x: x[1], reverse=True)
    return similarity_scores[:topn]


# === Èñ¢ÈÄ£Ë™ûÂ±ïÈñãÔºà1ÊÆµÈöéÔºâ ===
def expand_terms(terms: list[str], graph: nx.Graph, related_threshold: float = 0.35) -> dict:
    result = {}
    for term in terms:
        related = {
            neighbor: graph[term][neighbor]['weight']
            for neighbor in graph.neighbors(term)
            if graph[term][neighbor]['weight'] > related_threshold
        } if term in graph else {}
        result[term] = related
    return result



# Èñ¢ÈÄ£ÂõΩ„ÅÆÊ§úÁ¥¢
def expand_terms_with_w2v_fallback(
    terms: list[str],
    G: nx.Graph,
    w2v_model: KeyedVectors,
    loop_count: int = 100,
    threshold: float = 0.1
) -> dict[str, dict[str, list[str]]]:
    result = {}

    for term in terms:
        terms_2d = []
        irrelevant_terms = []

        # ÈÄöÂ∏∏„ÅØG„Åã„ÇâÂèñÂæó„ÄÅ„Å™„Åë„Çå„Å∞Word2Vec„Åã„ÇâÊé®ÂÆö
        if term in G:
            neighbors = list(G.neighbors(term))
        else:
            w2v_related = get_related_words_from_w2v(term, w2v_model, topn=5)
            neighbors = [w for w in w2v_related if w in G]

        if not neighbors:
            result[term] = {"related": [], "irrelevant": []}
            continue

        for _ in range(loop_count):
            pick_count = random.randint(1, min(10, len(neighbors)))
            picked_terms = random.sample(neighbors, pick_count)

            scores = {}
            for term2d in picked_terms:
                if term in G and term2d in G:
                    edge_data = G.get_edge_data(term, term2d)
                    score = edge_data['weight'] if edge_data and 'weight' in edge_data else 0.0
                else:
                    w2v_score = get_related_words_from_w2v(term, w2v_model).get(term2d, 0.0)
                    score = w2v_score

                # ÈñæÂÄ§„ÉÅ„Çß„ÉÉ„ÇØ
                if score >= threshold:
                    scores[term2d] = score
                else:
                    irrelevant_terms.append(term2d)

            # „Çπ„Ç≥„Ç¢Ë£úÊ≠£ÔºÜÂÜç„Éï„Ç£„É´„Çø
            for term2d in list(scores.keys()):
                if term2d in G and term in G:
                    inner_edge = G.get_edge_data(term2d, term)
                    inner_score = inner_edge['weight'] if inner_edge else 0.0
                else:
                    inner_score = get_related_words_from_w2v(term2d, w2v_model).get(term, 0.0)

                diff = abs(scores[term2d] - inner_score)
                scores[term2d] -= diff

                if scores[term2d] < threshold:
                    irrelevant_terms.append(term2d)
                    scores.pop(term2d)

            terms_2d.extend(scores.keys())

        result[term] = {"related": terms_2d, "irrelevant": list(set(irrelevant_terms))}

    return result


def run_two_phase_expansion(input_term: str, G: nx.Graph, w2v_model: KeyedVectors, logger=None):
    log = logger.info if logger else print
    result_set = set()

    log(f"[Á¨¨1„Éï„Çß„Éº„Ç∫] ÂÖ•ÂäõË™ûÂè•: {input_term}")

    # --- Á¨¨1„Éï„Çß„Éº„Ç∫Ôºöinput_term„ÅÆÊã°Âºµ ---
    phase1 = expand_terms_with_w2v_fallback([input_term], G, w2v_model, loop_count=100, threshold=0.35)
    for rel in phase1[input_term]["related"]:
        result_set.add(rel)

    # "Ë®ÄËëâ" „ÅÆÈñ¢ÈÄ£Ë™û„Çí„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Åã„ÇâÂèñÂæó„Åó„Å¶ result_set „Å´ËøΩÂä†
    if "Ë®ÄËëâ" in G:
        for neighbor in G.neighbors("Ë®ÄËëâ"):
            result_set.add(neighbor)
        log(f"[Á¨¨1„Éï„Çß„Éº„Ç∫] „ÄéË®ÄËëâ„Äè„Åã„Çâ„ÅÆÈñ¢ÈÄ£Ë™ûËøΩÂä†: {len(list(G.neighbors('Ë®ÄËëâ')))}Ë™û")
    else:
        log("‚ö†Ô∏è „ÄéË®ÄËëâ„Äè„ÅØ„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Å´Â≠òÂú®„Åó„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ")

    # --- Á¨¨2„Éï„Çß„Éº„Ç∫ÔºöÁ¨¨1„Éï„Çß„Éº„Ç∫„ÅÆÁµêÊûú„Çí term „Å®„Åó„Å¶ÂÜçÊã°Âºµ ---
    term_list = list(result_set)
    log(f"[Á¨¨2„Éï„Çß„Éº„Ç∫] Êã°ÂºµÂØæË±°Ë™ûÊï∞: {len(term_list)}")
    phase2 = expand_terms_with_w2v_fallback(term_list, G, w2v_model, loop_count=100, threshold=0.35)

    # --- Âá∫Âäõ ---
    for term, data in phase2.items():
        log(f"üåü {term}")
        log(f"  ‚úÖ related: {data['related'][:10]} ...")
        log(f"  üö´ irrelevant: {data['irrelevant'][:5]} ...")

    return phase2


def run_talk_command(text: str, G: nx.Graph, w2v_model, logger=None):

    log = logger.info if logger else print  # ‚úÖ ‰øÆÊ≠£
    log(f"[ÂÖ•Âäõ]: {text}")
    terms = segment_text_by_meaning_network(text, G)
    log(f"[ÂàÜÂâ≤Ë™ûÂè•]: {terms}")

    w2v_expanded_terms = set(terms)
    for term in terms:
        related_words = get_related_words_from_w2v(term, w2v_model, topn=5)
        w2v_expanded_terms.update(related_words.keys())
        

    log(f"Word2VecË£úÂº∑ÂæåË™ûÂè•: {list(w2v_expanded_terms)}")

    unknown_terms = [term for term in terms if term not in G]
    if unknown_terms:
        log("[Êú™Áü•Ë™û„ÅåÊ§úÂá∫„Åï„Çå„Åæ„Åó„Åü ‚Üí UnicodeÈ°û‰ººÂ∫¶„ÅßÈ°û‰ººË™ûÊé®ÂÆö]")
        known_terms = list(G.nodes)
        for unknown in unknown_terms:
            similar = estimate_similarity_by_unicode(unknown, known_terms, topn=5)
            log(f"  {unknown} ‚Üí {[f'{w}({round(s, 3)})' for w, s in similar]}")
            for word, score in similar:
                if not G.has_edge(unknown, word):
                    G.add_edge(unknown, word, weight=score)
                elif score > G[unknown][word]["weight"]:
                    G[unknown][word]["weight"] = score
    else:
        log("[Êú™Áü•Ë™û„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì]")

    related = expand_terms(terms, G)
    log("[Èñ¢ÈÄ£Ë™ûÂè•Ôºà„Åó„Åç„ÅÑÂÄ§ > 0.35Ôºâ]:")
    for term, rels in related.items():
        log(f"  {term}: {list(rels.keys())[:10]} ...")

    terms = list(set(terms) | w2v_expanded_terms)  # ÈáçË§áÊéíÈô§„Åó„Å¶„Éû„Éº„Ç∏


    # --- Á¨¨1„ÄúÊúÄÁµÇ„Éï„Çß„Éº„Ç∫ÁΩÆ„ÅçÊèõ„ÅàÔºàÊã°Âºµ + „Éï„Ç£„É´„ÇøÔºâ ---
    log("[Á¨¨1„Éï„Çß„Éº„Ç∫] ÂÖ•ÂäõË™ûÂè•„Å®ÊñáËÑàË™ûÂè•„ÅÆÊã°Âºµ (Word2Vec + „Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ)")

    result_set = set()

    # Á¨¨1„Éï„Çß„Éº„Ç∫‚ë†ÔºöÂÖ•ÂäõË™ûÂè•„ÇíÊã°Âºµ
    phase1 = expand_terms_with_w2v_fallback(terms, G, w2v_model, loop_count=100, threshold=0.35)
    for term, data in phase1.items():
        related = data["related"]
        irrelevant = data["irrelevant"]

        if related:
            log(f"  ‚úÖ {term}: {related[:10]} ...")
            result_set.update(related)
        if irrelevant:
            log(f"  üö´ {term} ÁÑ°Èñ¢‰øÇ„Å®Âà§ÂÆö„Åï„Çå„ÅüË™û: {irrelevant[:5]} ...")

    # Á¨¨1„Éï„Çß„Éº„Ç∫‚ë°Ôºö„ÄåË®ÄËëâ„Äç„ÄåË©±„Äç„ÄåË®ÄË™û„Äç„ÄåÁßÅ„Äç„ÇíÊã°Âºµ
    semantic_seeds = ["Ë®ÄËëâ", "Ë©±", "Ë®ÄË™û", "ÁßÅ"]
    semantic_expansion = expand_terms_with_w2v_fallback(semantic_seeds, G, w2v_model, loop_count=100, threshold=0.35)
    for seed_term, data in semantic_expansion.items():
        related = data["related"]
        irrelevant = data["irrelevant"]

        if related:
            log(f"  ‚úÖ {seed_term}ÔºàÊñáËÑàË™ûÔºâ: {related[:10]} ...")
            result_set.update(related)
        if irrelevant:
            log(f"  üö´ {seed_term}ÔºàÊñáËÑàË™ûÔºâ: ÁÑ°Èñ¢‰øÇ„Å®Âà§ÂÆö„Åï„Çå„ÅüË™û: {irrelevant[:5]} ...")

    # Á¨¨2„Éï„Çß„Éº„Ç∫Ôºö„Éû„Éº„Ç∏ÁµêÊûú„ÇíÂÜçÊã°Âºµ
    term_list = list(result_set)
    log(f"[Á¨¨2„Éï„Çß„Éº„Ç∫] Êã°ÂºµÂØæË±°Ë™ûÊï∞: {len(term_list)}Ë™û")
    phase2 = expand_terms_with_w2v_fallback(term_list, G, w2v_model, loop_count=100, threshold=0.35)

    all_related = set()
    for term, data in phase2.items():
        related = data["related"]
        irrelevant = data["irrelevant"]

        if related:
            log(f"  ‚úÖ {term}: {related[:10]} ...")
        if irrelevant:
            log(f"  üö´ {term}: ÁÑ°Èñ¢‰øÇË™û: {irrelevant[:5]} ...")

        for rel_term in related:
            all_related.add(rel_term)

            # „Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Å´ËøΩÂä†„Åæ„Åü„ÅØÂº∑Âåñ
            if not G.has_edge(term, rel_term):
                G.add_edge(term, rel_term, weight=0.5)
            elif G[term][rel_term]["weight"] < 0.5:
                G[term][rel_term]["weight"] = 0.5

    if all_related:
        log(f"[‚úî] ÊúÄÁµÇ„Éï„Çß„Éº„Ç∫„ÅßÂæó„Çâ„Çå„ÅüÈñ¢ÈÄ£Ë™ûÂè•ÔºàÁ∑èÊï∞: {len(all_related)}Ôºâ:")
        log(list(all_related))
    else:
        log("‚ö†Ô∏è ÊúÄÁµÇ„Éï„Çß„Éº„Ç∫„ÅßÊúâÂäπ„Å™Èñ¢ÈÄ£Ë™ûÂè•„ÅØË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ")

    # JSON‰øùÂ≠òÔºàÊÑèÂë≥„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÊõ¥Êñ∞Ôºâ
    save_data = {}
    for src, tgt, attrs in G.edges(data=True):
        save_data.setdefault(src, {})
        save_data[src][tgt] = round(attrs.get("weight", 0.0), 6)

    with open("meaning_network.json", "w", encoding="utf-8") as f:
        json.dump(save_data, f, ensure_ascii=False, indent=2)

    log("[‚úî] meaning_network.json „Å´Èñ¢ÈÄ£Â∫¶„Çí‰∏äÊõ∏„Åç‰øùÂ≠ò„Åó„Åæ„Åó„Åü„ÄÇ")

    # === „Éï„Ç°„Ç§„É´Âêç„Çí‰Ωø„Å£„Å¶„ÉÜ„Ç≠„Çπ„Éà‰øùÂ≠ò ===
    log("[üìÑ] Èñ¢ÈÄ£Ë™û„ÉªÁÑ°Èñ¢‰øÇË™û„Çí„ÉÜ„Ç≠„Çπ„Éà„Å®„Åó„Å¶‰øùÂ≠ò„Åó„Åæ„Åô„ÄÇ")

    os.makedirs("./memory/ÊúÄÈáçË¶ÅÈñ¢ÈÄ£Ë™û", exist_ok=True)
    merged_related = list(all_related)
    merged_irrelevant = []

    # Á¨¨2„Éï„Çß„Éº„Ç∫„ÅÆÁÑ°Èñ¢‰øÇË™û„ÇÇÂõûÂèé
    for data in phase2.values():
        merged_irrelevant.extend(data["irrelevant"])

    related_text = " ".join(merged_related)
    irrelevant_text = " ".join(set(merged_irrelevant))

    # ÊúÄÈáçË¶ÅÈñ¢ÈÄ£Ë™û„ÅÆÊ±∫ÂÆöÔºàÂ∑ÆÂàÜÊúÄÂ§ßÊñπÂºèÔºâ
    importance_scores = {}
    for word in merged_related:
        total_diff = 0.0
        for term in terms:
            if G.has_edge(term, word):
                weight = G[term][word].get("weight", 0.0)
            else:
                related_words = get_related_words_from_w2v(term, w2v_model, topn=5).get(word, 0.0)
                weight = get_related_words_from_w2v(term, w2v_model).get(word, 0.0)
            diff = abs(weight - 0.35)  # ÈñæÂÄ§„Å®„ÅÆÂ∑Æ
            total_diff += diff
        importance_scores[word] = total_diff

    most_important = max(importance_scores.items(), key=lambda x: x[1])[0] if importance_scores else "‰∏çÊòé"


    # ‰øùÂ≠ò
    filename = text.strip()
    path = f"./memory/ÊúÄÈáçË¶ÅÈñ¢ÈÄ£Ë™û/{filename}.txt"
    with open(path, "w", encoding="utf-8") as f:
        f.write(f"[ÊúÄÈáçË¶ÅÈñ¢ÈÄ£Ë™û]: {most_important}\n")
        f.write(f"[Èñ¢ÈÄ£Ë™û]:\n{related_text}\n")
        f.write(f"[ÁÑ°Èñ¢‰øÇË™û]:\n{irrelevant_text}\n")

    log(f"[‚úî] „ÉÜ„Ç≠„Çπ„Éà„Çí‰øùÂ≠ò„Åó„Åæ„Åó„Åü ‚Üí {path}")

def get_graph_and_model():
    G = load_meaning_network("meaning_network.json")
    return G, w2v_model


# === „É°„Ç§„É≥ÂæÖÊ©ü„É´„Éº„Éó ===
if __name__ == "__main__":
    G = load_meaning_network("meaning_network.json")
    print("„Ç≥„Éû„É≥„Éâ„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö")
    print("  /talk „ÉÜ„Ç≠„Çπ„Éà")
    print("  /looptest „ÉÜ„Ç≠„Çπ„Éà ÁØÑÂõ≤ ÂØÜÂ∫¶")
    print("  /exit")
    # === „É¢„Ç∏„É•„Éº„É´Áî®„Ç®„ÇØ„Çπ„Éù„Éº„Éà ===

    while True:
        try:
            cmd = input("> ").strip()

            if cmd == "/exit":
                print("ÁµÇ‰∫Ü„Åó„Åæ„Åô„ÄÇ")
                break

            elif cmd.startswith("/talk "):
                parts = cmd.split(maxsplit=1)
                if len(parts) < 2:
                    print("‚ö†Ô∏è /talk „ÅÆÂæå„Å´„ÉÜ„Ç≠„Çπ„Éà„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ")
                    continue
                _, text = parts
                run_talk_command(text, G)



            elif cmd.startswith("/looptest "):
                parts = cmd.split(maxsplit=4)
                if len(parts) != 4:
                    print("ÂΩ¢Âºè: /looptest <„ÉÜ„Ç≠„Çπ„Éà> <ÁØÑÂõ≤ÈñãÂßã> <ÂØÜÂ∫¶>")
                    continue
                _, text_input, base_str, step_str = parts
                base = float(base_str)
                step = float(step_str)
                values = [round(base + i * step, 3) for i in range(int((1.0 - base) / step) + 1)]

                for rel_th, sim_th in product(values, values):
                    result = segment_text_by_meaning_network(
                        text=text_input,
                        graph=G,
                        related_threshold=rel_th,
                        similarity_threshold=sim_th
                    )
                    print(f"[related={rel_th:.3f}, similarity={sim_th:.3f}] ‚Üí {result}")
                print("„É´„Éº„Éó„ÉÜ„Çπ„ÉàÂÆå‰∫Ü„ÄÇ")

        except Exception as e:
            print(f"„Ç®„É©„Éº: {e}")
