import json
import networkx as nx
from itertools import product
import random
import os
from gensim.models import KeyedVectors



# === æ„å‘³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯èª­ã¿è¾¼ã¿ ===
def load_meaning_network(json_path: str) -> nx.Graph:
    G = nx.Graph()
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
        for src, neighbors in data.items():
            for tgt, weight in neighbors.items():
                try:
                    G.add_edge(src, tgt, weight=float(weight))
                except ValueError:
                    continue
    return G



# Word2Vecãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ã«1åº¦ï¼‰
w2v_model = KeyedVectors.load_word2vec_format("model.vec", binary=False)

# é–¢é€£èªã‚’5èªã ã‘å–å¾—
def get_related_words_from_w2v(word, w2v_model, topn=5):
    try:
        results = w2v_model.most_similar(positive=[word], topn=topn)
        return {w: float(sim) for w, sim in results}
    except KeyError:
        return {}


# === åˆ†å‰²é–¢æ•° ===
def segment_text_by_meaning_network(
    text: str,
    graph: nx.Graph,
    n_min: int = 1,
    n_max: int = 6,
    related_threshold: float = 0.45,
    similarity_threshold: float = 0.6
) -> list[str]:
    def get_filtered_related_terms(term: str) -> set:
        if term not in graph:
            return set()
        return {
            neighbor
            for neighbor in graph.neighbors(term)
            if graph[term][neighbor].get("weight", 1.0) > related_threshold
        }

    all_matrices = []
    for n in range(n_min, n_max + 1):
        ngrams = [text[i:i+n] for i in range(len(text) - n + 1)]
        matrix = [get_filtered_related_terms(term) for term in ngrams]
        all_matrices.append(matrix)

    max_len = max(len(m) for m in all_matrices)
    break_scores = []
    for i in range(max_len - 1):
        similarities = []
        for matrix in all_matrices:
            if i < len(matrix) - 1:
                a, b = matrix[i], matrix[i+1]
                if a and b:
                    inter = a & b
                    union = a | b
                    sim = len(inter) / len(union) if union else 0.0
                    similarities.append(sim)
        break_score = 1.0 - (sum(similarities) / len(similarities)) if similarities else 1.0
        break_scores.append(break_score)

    result = []
    buffer = text[0]
    for i in range(1, len(text)):
        if i-1 < len(break_scores) and break_scores[i-1] >= similarity_threshold:
            result.append(buffer)
            buffer = text[i]
        else:
            buffer += text[i]
    result.append(buffer)

    return result

def random_chain_evaluation(graph, start_terms, steps=20, related_threshold=0.35):
    result = {}

    for start in start_terms:
        if start not in graph:
            continue

        valid_ends = []

        try:
            neighbors1 = list(graph.neighbors(start))
        except:
            continue  # ç„¡åŠ¹ãªèª

        for neighbor in neighbors1:
            current = neighbor
            for _ in range(steps):
                try:
                    next_neighbors = list(graph.neighbors(current))
                    if not next_neighbors:
                        break
                    current = random.choice(next_neighbors)
                except:
                    break

            # æœ€å¾Œã® current ãŒ start ã«ã¤ãªãŒã£ã¦ã„ã‚Œã°é‡ã¿è©•ä¾¡
            if graph.has_edge(start, current):
                weight = graph[start][current].get("weight", 0)
                if weight >= related_threshold:
                    valid_ends.append((current, round(weight, 3)))
                if not graph.has_edge(start, current):
                    graph.add_edge(start, current, weight=weight)
                elif weight > graph[start][current]["weight"]:
                    graph[start][current]["weight"] = weight

        result[start] = valid_ends

    return result

def evaluate_term_scores(
    targets: list[str],        # è©•ä¾¡å¯¾è±¡ã®èªãƒªã‚¹ãƒˆ
    reference_terms: list[str],  # è©•ä¾¡é …ç›®ï¼ˆä¸­å¿ƒèªãªã©ï¼‰
    G: nx.Graph,                # æ„å‘³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    w2v_model,                  # Word2Vecãƒ¢ãƒ‡ãƒ«
    base_weight: float = 0.35   # å·®åˆ†è©•ä¾¡ã®åŸºæº–ç‚¹ï¼ˆä¸­å¿ƒæ€§åŸºæº–ï¼‰
) -> dict[str, float]:
    """
    å„å¯¾è±¡èªã«ã¤ã„ã¦ã€è©•ä¾¡èªï¼ˆä¸­å¿ƒèªãªã©ï¼‰ã¨ã®é–¢é€£åº¦ã¨ã®å·®åˆ†ï¼ˆdiffï¼‰ã‚’è©•ä¾¡ã‚¹ã‚³ã‚¢ã¨ã™ã‚‹ã€‚
    ã‚¹ã‚³ã‚¢ãŒä½ã„ã»ã©ã€Œä¸­å¿ƒæ€§ãŒé«˜ã„ï¼ˆé‡è¦ï¼‰ã€ã¨åˆ¤æ–­ã•ã‚Œã‚‹ã€‚

    Returns:
        dict[str, float]: {èª: ã‚¹ã‚³ã‚¢}ï¼ˆã‚¹ã‚³ã‚¢ãŒå°ã•ã„ã»ã©é‡è¦ï¼‰
    """
    scores = {}

    for term in targets:
        diffs = []
        for ref in reference_terms:
            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ or W2V ã‹ã‚‰é‡ã¿å–å¾—
            if G.has_edge(term, ref):
                weight = G[term][ref].get("weight", 0.0)
            else:
                weight = get_related_words_from_w2v(term, w2v_model).get(ref, 0.0)

            # é–¾å€¤ã¨ã®å·®åˆ†ã‚’è¨ˆç®—
            diff = abs(weight - base_weight)
            diffs.append(diff)

        # å¹³å‡diffã‚¹ã‚³ã‚¢ï¼ˆä¸­å¿ƒèªã¨ã®è·é›¢ã®åˆè¨ˆï¼‰
        scores[term] = sum(diffs) / len(diffs) if diffs else float("inf")

    return scores



def estimate_similarity_by_unicode(unknown_term, known_terms, topn=5):
    def char_distance(c1, c2):
        return abs(ord(c1) - ord(c2))

    def term_distance(t1, t2):
        min_len = min(len(t1), len(t2))
        distances = [char_distance(t1[i], t2[i]) for i in range(min_len)]
        return sum(distances) / len(distances)

    similarity_scores = []
    for known in known_terms:
        dist = term_distance(unknown_term, known)
        similarity = 1 / (1 + dist)  # è·é›¢ã‚’é¡ä¼¼åº¦ã«å¤‰æ›
        similarity_scores.append((known, similarity))

    # é¡ä¼¼åº¦ã®é«˜ã„é †ã«ä¸¦ã¹ã‚‹
    similarity_scores.sort(key=lambda x: x[1], reverse=True)
    return similarity_scores[:topn]


# === é–¢é€£èªå±•é–‹ï¼ˆ1æ®µéšï¼‰ ===
def expand_terms(terms: list[str], graph: nx.Graph, related_threshold: float = 0.35) -> dict:
    result = {}
    for term in terms:
        related = {
            neighbor: graph[term][neighbor]['weight']
            for neighbor in graph.neighbors(term)
            if graph[term][neighbor]['weight'] > related_threshold
        } if term in graph else {}
        result[term] = related
    return result



# é–¢é€£å›½ã®æ¤œç´¢
def expand_terms_with_w2v_fallback(
    terms: list[str],
    G: nx.Graph,
    w2v_model: KeyedVectors,
    loop_count: int = 100,
    threshold: float = 0.1
) -> dict[str, dict[str, list[str]]]:
    result = {}

    for term in terms:
        terms_2d = []
        irrelevant_terms = []

        # é€šå¸¸ã¯Gã‹ã‚‰å–å¾—ã€ãªã‘ã‚Œã°Word2Vecã‹ã‚‰æ¨å®š
        if term in G:
            neighbors = list(G.neighbors(term))
        else:
            w2v_related = get_related_words_from_w2v(term, w2v_model, topn=5)
            neighbors = [w for w in w2v_related if w in G]

        if not neighbors:
            result[term] = {"related": [], "irrelevant": []}
            continue

        for _ in range(loop_count):
            pick_count = random.randint(1, min(10, len(neighbors)))
            picked_terms = random.sample(neighbors, pick_count)

            scores = {}
            for term2d in picked_terms:
                if term in G and term2d in G:
                    edge_data = G.get_edge_data(term, term2d)
                    score = edge_data['weight'] if edge_data and 'weight' in edge_data else 0.0
                else:
                    w2v_score = get_related_words_from_w2v(term, w2v_model).get(term2d, 0.0)
                    score = w2v_score

                # é–¾å€¤ãƒã‚§ãƒƒã‚¯
                if score >= threshold:
                    scores[term2d] = score
                else:
                    irrelevant_terms.append(term2d)

            # ã‚¹ã‚³ã‚¢è£œæ­£ï¼†å†ãƒ•ã‚£ãƒ«ã‚¿
            for term2d in list(scores.keys()):
                if term2d in G and term in G:
                    inner_edge = G.get_edge_data(term2d, term)
                    inner_score = inner_edge['weight'] if inner_edge else 0.0
                else:
                    inner_score = get_related_words_from_w2v(term2d, w2v_model).get(term, 0.0)

                diff = abs(scores[term2d] - inner_score)
                scores[term2d] -= diff

                if scores[term2d] < threshold:
                    irrelevant_terms.append(term2d)
                    scores.pop(term2d)

            terms_2d.extend(scores.keys())

        result[term] = {"related": terms_2d, "irrelevant": list(set(irrelevant_terms))}

    return result


def run_two_phase_expansion(input_term: str, G: nx.Graph, w2v_model: KeyedVectors, logger=None):
    log = logger.info if logger else print
    result_set = set()

    log(f"[ç¬¬1ãƒ•ã‚§ãƒ¼ã‚º] å…¥åŠ›èªå¥: {input_term}")

    # --- ç¬¬1ãƒ•ã‚§ãƒ¼ã‚ºï¼šinput_termã®æ‹¡å¼µ ---
    phase1 = expand_terms_with_w2v_fallback([input_term], G, w2v_model, loop_count=100, threshold=0.35)
    for rel in phase1[input_term]["related"]:
        result_set.add(rel)

    # "è¨€è‘‰" ã®é–¢é€£èªã‚’ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‹ã‚‰å–å¾—ã—ã¦ result_set ã«è¿½åŠ 
    if "è¨€è‘‰" in G:
        for neighbor in G.neighbors("è¨€è‘‰"):
            result_set.add(neighbor)
        log(f"[ç¬¬1ãƒ•ã‚§ãƒ¼ã‚º] ã€è¨€è‘‰ã€ã‹ã‚‰ã®é–¢é€£èªè¿½åŠ : {len(list(G.neighbors('è¨€è‘‰')))}èª")
    else:
        log("âš ï¸ ã€è¨€è‘‰ã€ã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«å­˜åœ¨ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")

    # --- ç¬¬2ãƒ•ã‚§ãƒ¼ã‚ºï¼šç¬¬1ãƒ•ã‚§ãƒ¼ã‚ºã®çµæœã‚’ term ã¨ã—ã¦å†æ‹¡å¼µ ---
    term_list = list(result_set)
    log(f"[ç¬¬2ãƒ•ã‚§ãƒ¼ã‚º] æ‹¡å¼µå¯¾è±¡èªæ•°: {len(term_list)}")
    phase2 = expand_terms_with_w2v_fallback(term_list, G, w2v_model, loop_count=100, threshold=0.35)

    # --- å‡ºåŠ› ---
    for term, data in phase2.items():
        log(f"ğŸŒŸ {term}")
        log(f"  âœ… related: {data['related'][:10]} ...")
        log(f"  ğŸš« irrelevant: {data['irrelevant'][:5]} ...")

    return phase2


def run_talk_command(text: str, G: nx.Graph, w2v_model, logger=None):

    log = logger.info if logger else print  # âœ… ä¿®æ­£
    log(f"[å…¥åŠ›]: {text}")
    terms = segment_text_by_meaning_network(text, G)
    log(f"[åˆ†å‰²èªå¥]: {terms}")

    w2v_expanded_terms = set(terms)
    for term in terms:
        related_words = get_related_words_from_w2v(term, w2v_model, topn=5)
        w2v_expanded_terms.update(related_words.keys())
        

    log(f"Word2Vecè£œå¼·å¾Œèªå¥: {list(w2v_expanded_terms)}")

    unknown_terms = [term for term in terms if term not in G]
    if unknown_terms:
        log("[æœªçŸ¥èªãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ â†’ Unicodeé¡ä¼¼åº¦ã§é¡ä¼¼èªæ¨å®š]")
        known_terms = list(G.nodes)
        for unknown in unknown_terms:
            similar = estimate_similarity_by_unicode(unknown, known_terms, topn=5)
            log(f"  {unknown} â†’ {[f'{w}({round(s, 3)})' for w, s in similar]}")
            for word, score in similar:
                if not G.has_edge(unknown, word):
                    G.add_edge(unknown, word, weight=score)
                elif score > G[unknown][word]["weight"]:
                    G[unknown][word]["weight"] = score
    else:
        log("[æœªçŸ¥èªã¯ã‚ã‚Šã¾ã›ã‚“]")

    related = expand_terms(terms, G)
    log("[é–¢é€£èªå¥ï¼ˆã—ãã„å€¤ > 0.35ï¼‰]:")
    for term, rels in related.items():
        log(f"  {term}: {list(rels.keys())[:10]} ...")

    terms = list(set(terms) | w2v_expanded_terms)  # é‡è¤‡æ’é™¤ã—ã¦ãƒãƒ¼ã‚¸


    # --- ç¬¬1ã€œæœ€çµ‚ãƒ•ã‚§ãƒ¼ã‚ºç½®ãæ›ãˆï¼ˆæ‹¡å¼µ + ãƒ•ã‚£ãƒ«ã‚¿ï¼‰ ---
    log("[ç¬¬1ãƒ•ã‚§ãƒ¼ã‚º] å…¥åŠ›èªå¥ã¨æ–‡è„ˆèªå¥ã®æ‹¡å¼µ (Word2Vec + ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯)")

    result_set = set()

    # ç¬¬1ãƒ•ã‚§ãƒ¼ã‚ºâ‘ ï¼šå…¥åŠ›èªå¥ã‚’æ‹¡å¼µ
    phase1 = expand_terms_with_w2v_fallback(terms, G, w2v_model, loop_count=100, threshold=0.35)
    for term, data in phase1.items():
        related = data["related"]
        irrelevant = data["irrelevant"]

        if related:
            log(f"  âœ… {term}: {related[:10]} ...")
            result_set.update(related)
        if irrelevant:
            log(f"  ğŸš« {term} ç„¡é–¢ä¿‚ã¨åˆ¤å®šã•ã‚ŒãŸèª: {irrelevant[:5]} ...")

    # ç¬¬1ãƒ•ã‚§ãƒ¼ã‚ºâ‘¡ï¼šã€Œè¨€è‘‰ã€ã€Œè©±ã€ã€Œè¨€èªã€ã€Œç§ã€ã‚’æ‹¡å¼µ
    semantic_seeds = ["è¨€è‘‰", "è©±", "è¨€èª", "ç§"]
    semantic_expansion = expand_terms_with_w2v_fallback(semantic_seeds, G, w2v_model, loop_count=100, threshold=0.35)
    for seed_term, data in semantic_expansion.items():
        related = data["related"]
        irrelevant = data["irrelevant"]

        if related:
            log(f"  âœ… {seed_term}ï¼ˆæ–‡è„ˆèªï¼‰: {related[:10]} ...")
            result_set.update(related)
        if irrelevant:
            log(f"  ğŸš« {seed_term}ï¼ˆæ–‡è„ˆèªï¼‰: ç„¡é–¢ä¿‚ã¨åˆ¤å®šã•ã‚ŒãŸèª: {irrelevant[:5]} ...")

    # ç¬¬2ãƒ•ã‚§ãƒ¼ã‚ºï¼šãƒãƒ¼ã‚¸çµæœã‚’å†æ‹¡å¼µ
    term_list = list(result_set)
    log(f"[ç¬¬2ãƒ•ã‚§ãƒ¼ã‚º] æ‹¡å¼µå¯¾è±¡èªæ•°: {len(term_list)}èª")
    phase2 = expand_terms_with_w2v_fallback(term_list, G, w2v_model, loop_count=100, threshold=0.35)

    all_related = set()
    for term, data in phase2.items():
        related = data["related"]
        irrelevant = data["irrelevant"]

        if related:
            log(f"  âœ… {term}: {related[:10]} ...")
        if irrelevant:
            log(f"  ğŸš« {term}: ç„¡é–¢ä¿‚èª: {irrelevant[:5]} ...")

        for rel_term in related:
            all_related.add(rel_term)

            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«è¿½åŠ ã¾ãŸã¯å¼·åŒ–
            if not G.has_edge(term, rel_term):
                G.add_edge(term, rel_term, weight=0.5)
            elif G[term][rel_term]["weight"] < 0.5:
                G[term][rel_term]["weight"] = 0.5

    if all_related:
        log(f"[âœ”] æœ€çµ‚ãƒ•ã‚§ãƒ¼ã‚ºã§å¾—ã‚‰ã‚ŒãŸé–¢é€£èªå¥ï¼ˆç·æ•°: {len(all_related)}ï¼‰:")
        log(list(all_related))
    else:
        log("âš ï¸ æœ€çµ‚ãƒ•ã‚§ãƒ¼ã‚ºã§æœ‰åŠ¹ãªé–¢é€£èªå¥ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    # JSONä¿å­˜ï¼ˆæ„å‘³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ›´æ–°ï¼‰
    save_data = {}
    for src, tgt, attrs in G.edges(data=True):
        save_data.setdefault(src, {})
        save_data[src][tgt] = round(attrs.get("weight", 0.0), 6)

    with open("meaning_network.json", "w", encoding="utf-8") as f:
        json.dump(save_data, f, ensure_ascii=False, indent=2)

    log("[âœ”] meaning_network.json ã«é–¢é€£åº¦ã‚’ä¸Šæ›¸ãä¿å­˜ã—ã¾ã—ãŸã€‚")

    # === ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½¿ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆä¿å­˜ ===
    log("[ğŸ“„] é–¢é€£èªãƒ»ç„¡é–¢ä¿‚èªã‚’ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚")

    os.makedirs("./memory/æœ€é‡è¦é–¢é€£èª", exist_ok=True)
    merged_related = list(all_related)
    merged_irrelevant = []

    # ç¬¬2ãƒ•ã‚§ãƒ¼ã‚ºã®ç„¡é–¢ä¿‚èªã‚‚å›å
    for data in phase2.values():
        merged_irrelevant.extend(data["irrelevant"])

    related_text = " ".join(merged_related)
    irrelevant_text = " ".join(set(merged_irrelevant))

    # æœ€é‡è¦é–¢é€£èªã®æ±ºå®šï¼ˆå·®åˆ†æœ€å¤§æ–¹å¼ï¼‰
    importance_scores = {}
    for word in merged_related:
        total_diff = 0.0
        for term in terms:
            if G.has_edge(term, word):
                weight = G[term][word].get("weight", 0.0)
            else:
                related_words = get_related_words_from_w2v(term, w2v_model, topn=5).get(word, 0.0)
                weight = get_related_words_from_w2v(term, w2v_model).get(word, 0.0)
            diff = abs(weight - 0.35)  # é–¾å€¤ã¨ã®å·®
            total_diff += diff
        importance_scores[word] = total_diff

    most_important = max(importance_scores.items(), key=lambda x: x[1])[0] if importance_scores else "ä¸æ˜"


    # ä¿å­˜
    filename = text.strip()
    path = f"./memory/æœ€é‡è¦é–¢é€£èª/{filename}.txt"
    with open(path, "w", encoding="utf-8") as f:
        f.write(f"[æœ€é‡è¦é–¢é€£èª]: {most_important}\n")
        f.write(f"[é–¢é€£èª]:\n{related_text}\n")
        f.write(f"[ç„¡é–¢ä¿‚èª]:\n{irrelevant_text}\n")

    log(f"[âœ”] ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ â†’ {path}")

def get_graph_and_model():
    G = load_meaning_network("meaning_network.json")
    return G, w2v_model


# === ãƒ¡ã‚¤ãƒ³å¾…æ©Ÿãƒ«ãƒ¼ãƒ— ===
if __name__ == "__main__":
    G = load_meaning_network("meaning_network.json")
    print("ã‚³ãƒãƒ³ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š")
    print("  /talk ãƒ†ã‚­ã‚¹ãƒˆ")
    print("  /looptest ãƒ†ã‚­ã‚¹ãƒˆ ç¯„å›² å¯†åº¦")
    print("  /exit")
    # === ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ç”¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ ===

    while True:
        try:
            cmd = input("> ").strip()

            if cmd == "/exit":
                print("çµ‚äº†ã—ã¾ã™ã€‚")
                break

            elif cmd.startswith("/talk "):
                parts = cmd.split(maxsplit=1)
                if len(parts) < 2:
                    print("âš ï¸ /talk ã®å¾Œã«ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                    continue
                _, text = parts
                run_talk_command(text, G)



            elif cmd.startswith("/looptest "):
                parts = cmd.split(maxsplit=4)
                if len(parts) != 4:
                    print("å½¢å¼: /looptest <ãƒ†ã‚­ã‚¹ãƒˆ> <ç¯„å›²é–‹å§‹> <å¯†åº¦>")
                    continue
                _, text_input, base_str, step_str = parts
                base = float(base_str)
                step = float(step_str)
                values = [round(base + i * step, 3) for i in range(int((1.0 - base) / step) + 1)]

                for rel_th, sim_th in product(values, values):
                    result = segment_text_by_meaning_network(
                        text=text_input,
                        graph=G,
                        related_threshold=rel_th,
                        similarity_threshold=sim_th
                    )
                    print(f"[related={rel_th:.3f}, similarity={sim_th:.3f}] â†’ {result}")
                print("ãƒ«ãƒ¼ãƒ—ãƒ†ã‚¹ãƒˆå®Œäº†ã€‚")

        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: {e}")
